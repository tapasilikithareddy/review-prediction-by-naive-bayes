{
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = ':https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F18%2F2157%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240525%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240525T171939Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D640f2d319923929f2b5efa6ffe4fa57a4fbc0e0cc69e211da54bc24be7b2ea043ffa66b97cbe6b51142b66f7dff06c664ea08c95118e6bbc7b98f84265a76f8dd37c4abc01a71ce21858755ee63a468750d3369c2dc67d0875a3c40cd141356f5962ed7a73c3c620cd0b7defce7568c5e8aec96eed98ce0c0cc801d481460c8a07a8b72725da3b6f78c1a7f26b6e539dafef5e72fd0d9403eed7ac06fe70b27b5b9878509930a8ff018e00e774ca87492a7ebc6bd928c9ec44c18e724352cc25a7e980fa56d1416269bc8948255c12e8c4e6958f09e898c3dd5604ca96dcead6f370587139ab05fd6cba783d447f538714d59453b40522a755f634501061c4e7'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "d8lc19Tfw_dG"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "097e0a82ed826a5f638e80cd167d171638334dac",
        "id": "OJ2GhU6Aw_dK"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 Introduction\n",
        "This python notebook for Amazon food reviews polarity prediction based on the given review data by applying Naive Bayes algorithm which is based on Bayes probability model. To build generalized prediction model first step should be necessary cleaning of data as a part of data preprocessing.\n",
        "\n",
        "We will perform following data preprocessing.\n",
        "\n",
        "* Removing Stop-words\n",
        "* Remove any punctuations or limited set of special characters like , or . or # etc.\n",
        "* Snowball Stemming the word\n",
        "* Convert the word to lowercase\n",
        "\n",
        "Once we the data is cleaned to be processed we'll use below Feature generation techniques to convert text to numeric vector.\n",
        "1. Bag Of Words (BoW)\n",
        "1. Term Frequency - inverse document frequency (tf-idf)\n",
        "\n",
        "Using Naive Bayes algorithm we will build model to predict review polarity for each technique.\n",
        "\n",
        "**Objective:** Given a review determine whether a review is positive or negative, by appling Naive Bayes algorithm and deciding the best Feature generation technique with most important features for positive & negative class. We will generate ROC curve for each model to check the sensibility of model\n",
        "\n",
        "**Note:** As word2vec generate vector with negative valuel we can not use it for Naive Bays algorithm as it accepts positive values only as a imput vector.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "36594d6fad8af9a0a3cf1ea96aa88f49b96899a1",
        "id": "PdKFLgSNw_dL"
      },
      "cell_type": "markdown",
      "source": [
        "**1.1 Load and check data**"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "collapsed": true,
        "id": "S_Jt0s4vw_dM"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot.metrics as skplt\n",
        "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import seaborn as sn\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"../input\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true,
        "id": "uZwMd7_Gw_dM"
      },
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "con = sqlite3.connect('../input/database.sqlite')\n",
        "\n",
        "filtered_data = pd.read_sql_query(\"\"\"select * from Reviews WHERE Score != 3\"\"\",con)\n",
        "\n",
        "filtered_data.shape\n",
        "filtered_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "acca4aaae629afb66f5046f11922610cd0593928",
        "id": "BNNwaLyVw_dN"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "_uuid": "7166ffed9e1d5eac70a43afb646bbe40d3546069",
        "id": "qsVbtD09w_dN"
      },
      "cell_type": "markdown",
      "source": [
        "*  **Segregating data as positive and negative**"
      ]
    },
    {
      "metadata": {
        "_uuid": "0b8882761075bfb280c2baed771798a6d6b3a70a",
        "trusted": false,
        "collapsed": true,
        "id": "FkPySMr5w_dO"
      },
      "cell_type": "code",
      "source": [
        "# Here are replacing review score 1,2 as negative and 4,5 as a positive. we are skipping review score 3 considering it as a neutral.\n",
        "def partition(x):\n",
        "    if x<3:\n",
        "        return 'negative'\n",
        "    return 'positive'\n",
        "\n",
        "actualScore = filtered_data['Score']\n",
        "positiveNegative = actualScore.map(partition)\n",
        "filtered_data['Score'] = positiveNegative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd49f3708690782d127d05d1869ae17a2e01c7da",
        "id": "3ShSQssJw_dO"
      },
      "cell_type": "markdown",
      "source": [
        "* **Sorting data for time based splitting for model train and test dataset**"
      ]
    },
    {
      "metadata": {
        "_uuid": "b83c0b4f66747d79241cee4b7fd7721f9e4ab655",
        "trusted": false,
        "collapsed": true,
        "id": "obf987skw_dO"
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "filtered_data[\"Time\"] = filtered_data[\"Time\"].map(lambda t: datetime.datetime.fromtimestamp(int(t)).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "sortedData = filtered_data.sort_values('ProductId',axis=0,kind=\"quicksort\", ascending=True)\n",
        "final = sortedData.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep=\"first\",inplace=False)\n",
        "\n",
        "final = final[final.HelpfulnessNumerator <= final.HelpfulnessDenominator]\n",
        "\n",
        "#As data is huge, due to computation limitation we will randomly select data. we will try to pick data in a way so that it doesn't make data imbalance problem\n",
        "finalp = final[final.Score == 'positive']\n",
        "finalp = finalp.sample(frac=0.2,random_state=1) #0.055\n",
        "\n",
        "finaln = final[final.Score == 'negative']\n",
        "finaln = finaln.sample(frac=0.9,random_state=1) #0.25\n",
        "\n",
        "final = pd.concat([finalp,finaln],axis=0)\n",
        "\n",
        "#sording data by timestamp so that it can be devided in train and test dataset for time based slicing.\n",
        "final = final.sort_values('Time',axis=0,kind=\"quicksort\", ascending=True).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(final.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7e0f9ae4596b49535783b2b09ca9e9427c2519c1",
        "trusted": false,
        "collapsed": true,
        "id": "zsWVLQq2w_dO"
      },
      "cell_type": "code",
      "source": [
        "final['Score'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "618f98aead226aef9ca5c02dc0063a0adea0839c",
        "id": "xmmo9qTcw_dP"
      },
      "cell_type": "markdown",
      "source": [
        "* ** Removing Stop-words **\n",
        "* ** Remove any punctuations or limited set of special characters like , or . or # etc. **\n",
        "* ** Snowball Stemming the word **\n",
        "* ** Convert the word to lowercase **"
      ]
    },
    {
      "metadata": {
        "_uuid": "3d9b2af993d32742fc3a0847f651a14b207d24b0",
        "trusted": false,
        "collapsed": true,
        "id": "m2p8OZTPw_dP"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "stop = set(stopwords.words('english')) #set of stopwords\n",
        "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
        "\n",
        "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', sentence)\n",
        "    return cleantext\n",
        "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    return  cleaned\n",
        "\n",
        "i=0\n",
        "str1=' '\n",
        "final_string=[]\n",
        "all_positive_words=[] # store words from +ve reviews here\n",
        "all_negative_words=[] # store words from -ve reviews here.\n",
        "s=''\n",
        "\n",
        "final_string=[]\n",
        "all_positive_words=[] # store words from +ve reviews here\n",
        "all_negative_words=[] # store words from -ve reviews here.\n",
        "s=''\n",
        "for sent in final['Text'].values:\n",
        "    filtered_sentence=[]\n",
        "    #print(sent);\n",
        "    sent=cleanhtml(sent) # remove HTMl tags\n",
        "    for w in sent.split():\n",
        "        for cleaned_words in cleanpunc(w).split():\n",
        "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n",
        "                if(cleaned_words.lower() not in stop):\n",
        "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
        "                    filtered_sentence.append(s)\n",
        "                    if (final['Score'].values)[i] == 'positive':\n",
        "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
        "                    if(final['Score'].values)[i] == 'negative':\n",
        "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                continue\n",
        "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
        "    final_string.append(str1)\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f7494f02aaeda38074d53411faeaebcfe5a9fc6e",
        "id": "Uq3skxTew_dP"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 Building function to find optimal Alpha for Naive Bayes"
      ]
    },
    {
      "metadata": {
        "_uuid": "6a4a9ef114e73eb6c595a4016d5869c4b0d1e609",
        "id": "SusN5mW7w_dP"
      },
      "cell_type": "markdown",
      "source": [
        "**To Find the optimal alpha we will used cross validation method. Based on misclassifiction error for with different alpha, we will decide the best alpha on Train Data**"
      ]
    },
    {
      "metadata": {
        "_uuid": "fd5263f093804728e845f3f05cced352afb257ca",
        "trusted": false,
        "collapsed": true,
        "id": "nakNhcrjw_dP"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import cross_validation\n",
        "\n",
        "\n",
        "def find_optimal_k(X_train,y_train, myList):\n",
        "\n",
        "    # empty list that will hold cv scores\n",
        "    cv_scores = []\n",
        "\n",
        "    # split the train data set into cross validation train and cross validation test\n",
        "    X_tr, X_cv, y_tr, y_cv = cross_validation.train_test_split(X_train, y_train, test_size=0.3)\n",
        "\n",
        "    for i in myList:\n",
        "        nb = MultinomialNB(alpha = i)\n",
        "        model = nb.fit(X_tr, y_tr)\n",
        "\n",
        "        # predict the response on the crossvalidation train\n",
        "        pred = model.predict(X_cv)\n",
        "\n",
        "        # evaluate CV accuracy\n",
        "        acc = accuracy_score(y_cv, pred, normalize=True)\n",
        "        cv_scores.append(acc)\n",
        "\n",
        "    # changing to misclassification error\n",
        "    MSE = [1 - x for x in cv_scores]\n",
        "\n",
        "    # determining best alpha\n",
        "    optimal_alpha = myList[MSE.index(min(MSE))]\n",
        "    print('\\nThe optimal alpha is ', optimal_alpha)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(myList,MSE,color='blue', linestyle='dashed', marker='o',\n",
        "             markerfacecolor='red', markersize=10)\n",
        "    plt.title('Error Rate vs. alpha Value')\n",
        "    plt.xlabel('alpha')\n",
        "    plt.ylabel('Error Rate')\n",
        "\n",
        "    print(\"the misclassification error for each k value is : \", np.round(MSE,3))\n",
        "\n",
        "    return optimal_alpha\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "66e54b5ab7cb08c985f3328b672b8aeaa8608511",
        "id": "1IM5LVZmw_dP"
      },
      "cell_type": "markdown",
      "source": [
        "# 4 Feature generation techniques to convert text to numeric vector.[](http://)"
      ]
    },
    {
      "metadata": {
        "_uuid": "dff1026170747c5eb9fb264d0ab8cc0cce2e3430",
        "id": "2rxPoh5ew_dP"
      },
      "cell_type": "markdown",
      "source": [
        "# 4.1 Appling Naive Bayes with BoW"
      ]
    },
    {
      "metadata": {
        "_uuid": "4d80c345777aab7cec0f599ed21ccabbd82b041b",
        "id": "B8J2Pc-5w_dQ"
      },
      "cell_type": "markdown",
      "source": [
        "**Generating Bag of Wrods Vector matrix for Reviews**"
      ]
    },
    {
      "metadata": {
        "_uuid": "2534297d4f9d9073c6b885e69c9551d0ec0657ab",
        "trusted": false,
        "collapsed": true,
        "id": "SLZRfbbnw_dQ"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#count_vect = CountVectorizer(ngram_range=(1,2) )\n",
        "count_vect = CountVectorizer()\n",
        "final_bow_count = count_vect.fit_transform(final_string)#final['Text'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "83bd7bbca2e6451b7b31bfdb7a8e64e62b4a0655",
        "trusted": false,
        "collapsed": true,
        "id": "KXhgF5hNw_dQ"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "final_bow_np = StandardScaler(with_mean=False).fit_transform(final_bow_count )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f91095bba1e16ae4eb7ebbc5dd72bf43aad9d583",
        "id": "yRQfhCJPw_dQ"
      },
      "cell_type": "markdown",
      "source": [
        "**Splitting Data into Train and Test based on the timestamp of review**"
      ]
    },
    {
      "metadata": {
        "_uuid": "e377024a4c25873a082f982a5815e05e062bd58d",
        "trusted": false,
        "collapsed": true,
        "id": "0fxlOih3w_dQ"
      },
      "cell_type": "code",
      "source": [
        "#We already have sorted data by timestamp so we will use first 70% of data as Train with cross validation and next 30% for test\n",
        "import math\n",
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X = final_bow_np\n",
        "y = final['Score']\n",
        "\n",
        "X_train =  final_bow_np[:math.ceil(len(final)*.7)]\n",
        "X_test = final_bow_np[math.ceil(len(final)*.7):]\n",
        "y_train = y[:math.ceil(len(final)*.7)]\n",
        "y_test =  y[math.ceil(len(final)*.7):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d116b716fca884f392cbd58626c5b999872fe059",
        "id": "cSyLkmQaw_dQ"
      },
      "cell_type": "markdown",
      "source": [
        "**Finding Optimal alpha Cross validation**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "7fd5ad8c57f0d225548d99b8dc6f46ab82eef5a4",
        "collapsed": true,
        "id": "T_ftc_rXw_dQ"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "myList = np.arange(0.00001, 0.001, 0.00005) #list(range(1,200))\n",
        "optimal_alpha = find_optimal_k(X_train ,y_train,myList)\n",
        "\n",
        "print(optimal_alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d2b991681294a3c28fc327e2927e1a33324ce406",
        "id": "Hd0WFoYzw_dQ"
      },
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes with Optimal alpha**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "ab28317206777e0533eeb358efe5b4c458ad5224",
        "id": "q2NQr6gHw_dR"
      },
      "cell_type": "code",
      "source": [
        "nb = MultinomialNB(alpha = optimal_alpha)\n",
        "model = nb.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "07ff7624d06da76a3ab0c081f599d857427a83b8",
        "id": "kVxECnnSw_dR"
      },
      "cell_type": "code",
      "source": [
        "predbow = (model.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "1c98fe7c4c566bb80e6647a622fe2359ccf8d6d1",
        "id": "iJEfd3faw_dR"
      },
      "cell_type": "code",
      "source": [
        "predbowprob = model.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "27248f4cbaecf30c487ce6248d8e463220cce952",
        "trusted": false,
        "collapsed": true,
        "id": "YVkEtiEjw_dR"
      },
      "cell_type": "code",
      "source": [
        "skplt.plot_confusion_matrix(y_test ,predbow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f3f72fa0805282e8f0f693509e62cf86830d7f30",
        "trusted": false,
        "collapsed": true,
        "id": "ytkcGihow_dR"
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test ,predbow))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "0a39bbb432075c2bd22ad1eb14ecf071628fa968",
        "collapsed": true,
        "id": "7GZFgGE7w_dR"
      },
      "cell_type": "code",
      "source": [
        "print(\"Accuracy for Naive Bayes model with Bag of Words is \",round(accuracy_score(y_test ,predbow),3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "86ec028915a39cc2b6a45e5b92778046df3b3a96",
        "id": "Z4spQWCrw_dR"
      },
      "cell_type": "markdown",
      "source": [
        "**Feature Importance**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "a7b2f5815aa114dbbc3ab2eae0474a34252b9a65",
        "id": "CX2j3P8sw_dR"
      },
      "cell_type": "code",
      "source": [
        "bow_feat = count_vect.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "dc17c7688b42025b19b7e93a7fd935b6e335ebe6",
        "collapsed": true,
        "id": "4hOOHLTfw_dR"
      },
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "ed7e71ed0b06c0b720ce4468407c4b7a3690de23",
        "collapsed": true,
        "id": "BroCweBtw_dS"
      },
      "cell_type": "code",
      "source": [
        "model.class_count_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "b009b4498a7cc0b9a49d2bd7385324352ebf37f9",
        "id": "hsLhE6MNw_dS"
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(model.feature_log_prob_,columns=bow_feat)\n",
        "df1_transposed = df.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "b8abdfb9e88d0ea30a75d86b8c7aad2a558edfe6",
        "id": "Dt6LAh_mw_dS"
      },
      "cell_type": "code",
      "source": [
        "fe_bow_neg = df1_transposed[0].sort_values(ascending = False)[0:10] ##Negative\n",
        "fe_bow_pos =df1_transposed[1].sort_values(ascending = False)[0:10] ##Positive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bcec55b78349aa58be416084d28c140c7c0ebc96",
        "id": "AF0SMBtlw_dS"
      },
      "cell_type": "markdown",
      "source": [
        "**Top 10 words found in Negative reviews with high probability**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "16a9c35c938958f7a994467dbe811790b349c340",
        "collapsed": true,
        "id": "cmHoCI9Pw_dW"
      },
      "cell_type": "code",
      "source": [
        "fe_bow_neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4cb36a13220bfe9a17d2e84e0d176e5dfc4ef8e8",
        "id": "-y9ymoUNw_dW"
      },
      "cell_type": "markdown",
      "source": [
        "**Top 10 words found in Positive reviews with high probability**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "53d33204791e4472287314b905c4a3e906176dbc",
        "collapsed": true,
        "id": "Uj_QlUVYw_dW"
      },
      "cell_type": "code",
      "source": [
        "fe_bow_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "19f5f3e3c9e6112f69db97def25db09911c86a00",
        "id": "1K-q5Kvsw_dW"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "# Binarize the output\n",
        "y_bow = label_binarize(y_test, classes= [\"negative\",\"positive\",\"x\"])[:,:-1]\n",
        "n_classes = y_bow.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "d5bba32c0ccf08133e239c91caa73775de53d394",
        "id": "iq1bqnX6w_dW"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_bow[:, i], predbowprob[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bow.ravel(), predbowprob.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# First aggregate all false positive rates\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "from scipy import interp\n",
        "# Then interpolate all ROC curves at this points\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "# Finally average it and compute AUC\n",
        "mean_tpr /= n_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "42e08a417ad566da464ebdc6ef2fe0d52a71086a",
        "id": "V2jQz6pyw_dX"
      },
      "cell_type": "markdown",
      "source": [
        "# 4.2 Appling Naive Bayes with tf-idf"
      ]
    },
    {
      "metadata": {
        "_uuid": "e2f1e62f87b03767b0aa5f12475864e40b755d3d",
        "id": "MX5IoBXVw_dX"
      },
      "cell_type": "markdown",
      "source": [
        "**Generating tf-idf Vector matrix for Reviews**"
      ]
    },
    {
      "metadata": {
        "_uuid": "a389d48d31a532de466e83537d134a10fcbdac04",
        "trusted": false,
        "collapsed": true,
        "id": "Rb_Q4lKlw_dX"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_idf_vec = TfidfVectorizer()#ngram_range=(2,2))\n",
        "\n",
        "final_tfidf_count = tf_idf_vec.fit_transform(final_string)#final['Text'].values)\n",
        "\n",
        "#print(final_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "466ae065f620307662122d5a11282d7a850fa2ab",
        "id": "3Rxy34cyw_dX"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "final_tfidf_np = StandardScaler(with_mean=False).fit_transform(final_tfidf_count )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ea185a7cc29c8389c4c3de91b7bb63eb56a0ccb8",
        "id": "gr8wxk5Hw_dX"
      },
      "cell_type": "markdown",
      "source": [
        "**Splitting Data into Train and Test**"
      ]
    },
    {
      "metadata": {
        "_uuid": "2bcf16f27980f1af717dd6a369929554dd3ddea1",
        "trusted": false,
        "collapsed": true,
        "id": "BaPIKKO4w_dX"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = final_tfidf_np\n",
        "y = final['Score']\n",
        "\n",
        "X_train =  final_tfidf_np[:math.ceil(len(final)*.7)]\n",
        "X_test = final_tfidf_np[math.ceil(len(final)*.7):]\n",
        "y_train = y[:math.ceil(len(final)*.7)]\n",
        "y_test =  y[math.ceil(len(final)*.7):]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d116b716fca884f392cbd58626c5b999872fe059",
        "id": "6o0KJEaRw_dX"
      },
      "cell_type": "markdown",
      "source": [
        "**Finding Optimal alpha Cross validation**"
      ]
    },
    {
      "metadata": {
        "_uuid": "448ea0a73c760c12b77e2538c9ae3dbcad2f1fcc",
        "trusted": false,
        "collapsed": true,
        "id": "N4hBCVUfw_dX"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "myList = np.arange(0.00001, 0.001, 0.00005) #list(range(1,200))\n",
        "optimal_alpha = find_optimal_k(X_train ,y_train,myList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d2b991681294a3c28fc327e2927e1a33324ce406",
        "id": "zmMD-ZAXw_dX"
      },
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes with Optimal alpha**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "02aaf9dd5de72c11587e0087fd8364ec6161d648",
        "id": "9GhtpZTxw_dY"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb = MultinomialNB(optimal_alpha)\n",
        "model = nb.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "7ad899284443d42990f930ba1938f7e9aa92c79a",
        "id": "z5hKD-0vw_dY"
      },
      "cell_type": "code",
      "source": [
        "predtfidf = (model.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "e9a3a6b8739eee598e5129b42fcdf92748c14691",
        "id": "IqMAUmYTw_dY"
      },
      "cell_type": "code",
      "source": [
        "predtfidfprob = (model.predict_proba(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b8596006083d338423b54c422d9277ab31d68bc9",
        "trusted": false,
        "collapsed": true,
        "id": "yFQzNYMjw_dY"
      },
      "cell_type": "code",
      "source": [
        "skplt.plot_confusion_matrix(y_test ,predtfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "827727cff0a20345a23b046fbf0b42060ff1ff35",
        "trusted": false,
        "collapsed": true,
        "id": "DfaAyparw_dY"
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test ,predtfidf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "8fdb12b35b8d8a96a0c4f4c9dd6adb8debbd2f0d",
        "collapsed": true,
        "id": "-x39pm43w_dY"
      },
      "cell_type": "code",
      "source": [
        "print(\"Accuracy for Naive Bayes model with tf-id is \",round(accuracy_score(y_test ,predtfidf),3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "72d2bd8682dafb1771961d34a67554bc32aee861",
        "id": "gx5IJuLvw_dY"
      },
      "cell_type": "markdown",
      "source": [
        "**Feature Importance**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "fb79c0dd46fa7dfff6597d1d7b1b59b456046b51",
        "id": "TPwqFjHvw_dZ"
      },
      "cell_type": "code",
      "source": [
        "tfidf_feat = tf_idf_vec.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "5ae20daeb19967981fe16a4a308e89df58f54b05",
        "collapsed": true,
        "id": "foGoZdAKw_dZ"
      },
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "9549dc2bb07d6732ff39551b39758f3655f64e3d",
        "id": "TAMSY1pUw_dZ"
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(model.feature_log_prob_,columns=tfidf_feat)\n",
        "df1_transposed = df.T\n",
        "fe_tfidf_pos = df1_transposed[1].sort_values(ascending = False)[0:10] ##Positive\n",
        "fe_tfidf_neg = df1_transposed[0].sort_values(ascending = False)[0:10] ##Negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "96e2179d969a189ba9fdebc757a6913c6d5cda1f",
        "id": "hAes_YgYw_dZ"
      },
      "cell_type": "markdown",
      "source": [
        "**Top 10 words found in Negative reviews with high probability**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "b47bdaefd12640e70360414de3e22119198c25cb",
        "collapsed": true,
        "id": "aLTFyRvkw_dZ"
      },
      "cell_type": "code",
      "source": [
        "fe_tfidf_neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "83ee2652f44bc524e771f7d06df8e39e3388ddd6",
        "id": "ZhcvSLsSw_dZ"
      },
      "cell_type": "markdown",
      "source": [
        "**Top 10 words found in positive reviews with high probability**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "51995abf62a281652f874cc74d0f5ba14ea29222",
        "collapsed": true,
        "id": "i04kEECjw_dZ"
      },
      "cell_type": "code",
      "source": [
        "fe_tfidf_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "63a1c99d540495cdcedeeee1aafe044a4dc45b39",
        "id": "ea17cWXVw_dZ"
      },
      "cell_type": "markdown",
      "source": [
        "# 5 Observation"
      ]
    },
    {
      "metadata": {
        "_uuid": "1dfc9b1cf7cb7ec4fa2461dd31648a526c28e975",
        "id": "vEV3Qr7vw_da"
      },
      "cell_type": "markdown",
      "source": [
        "**At the end of the Analysis we have two Naive Bayes model to compare with different Feature generation techniques, one with BoW and second with tf-idf.\n",
        "By comparing accuracy of models it's clear that both the model have almost equal accuracy more than 75%.**\n"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "ef5b62526449c3641e94832d96542ed8c4b600ee",
        "id": "kQWDzQtJw_da"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "# Binarize the output\n",
        "y_tfidf = label_binarize(y_test, classes= [\"negative\",\"positive\",\"x\"])[:,:-1]\n",
        "n_classes = y_tfidf.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "bb63ba81f23f4cf2577a0c43a21c6de97a110733",
        "id": "Mdaso3Gpw_da"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr2 = dict()\n",
        "tpr2 = dict()\n",
        "roc_auc2 = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr2[i], tpr2[i], _ = roc_curve(y_tfidf[:, i], predtfidfprob[:, i])\n",
        "    roc_auc2[i] = auc(fpr2[i], tpr2[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr2[\"micro\"], tpr2[\"micro\"], _ = roc_curve(y_tfidf.ravel(), predtfidfprob.ravel())\n",
        "roc_auc2[\"micro\"] = auc(fpr2[\"micro\"], tpr2[\"micro\"])\n",
        "\n",
        "# First aggregate all false positive rates\n",
        "all_fpr2 = np.unique(np.concatenate([fpr2[i] for i in range(n_classes)]))\n",
        "\n",
        "from scipy import interp\n",
        "# Then interpolate all ROC curves at this points\n",
        "mean_tpr2 = np.zeros_like(all_fpr2)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr2 += interp(all_fpr2, fpr2[i], tpr2[i])\n",
        "\n",
        "# Finally average it and compute AUC\n",
        "mean_tpr2 /= n_classes\n",
        "\n",
        "fpr2[\"macro\"] = all_fpr2\n",
        "tpr2[\"macro\"] = mean_tpr2\n",
        "roc_auc2[\"macro\"] = auc(fpr2[\"macro\"], tpr2[\"macro\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "593cd344a434870eaba0e5725122356cf40efaea",
        "collapsed": true,
        "id": "v2xp226vw_da"
      },
      "cell_type": "code",
      "source": [
        "from itertools import cycle\n",
        "lw = 2\n",
        "f, (ax1 , ax2) = plt.subplots(1,2, sharex=True ,figsize=(15,7))\n",
        "##axarr[1].figure(figsize=(10,8))\n",
        "ax1 = plt.subplot(121)\n",
        "ax2 = plt.subplot(122)\n",
        "\n",
        "\n",
        "ax1.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4, )\n",
        "\n",
        "ax1.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    ax1.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "ax1.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "#ax1.axis([4, 8, 8, 20])\n",
        "ax1.set_xlim([0.0, 1.0])\n",
        "ax1.set_ylim([0.0, 1.05])\n",
        "ax1.set_xlabel('False Positive Rate')\n",
        "ax1.set_ylabel('True Positive Rate')\n",
        "ax1.title.set_text('Bag Of Words (BoW)')\n",
        "ax1.legend(loc=\"lower right\")\n",
        "\n",
        "\n",
        "ax2.plot(fpr2[\"micro\"], tpr2[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "ax2.plot(fpr2[\"macro\"], tpr2[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    ax2.plot(fpr2[i], tpr2[i], color=color, lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc2[i]))\n",
        "\n",
        "ax2.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "# ax2.axis([4, 8, 8, 20])\n",
        "ax2.set_xlim([0.0, 1.0])\n",
        "ax2.set_ylim([0.0, 1.05])\n",
        "ax2.set_xlabel('False Positive Rate')\n",
        "#ax2.set_ylabel('True Positive Rate')\n",
        "ax2.title.set_text('Term Frequency - inverse document frequency (tf-idf)')\n",
        "\n",
        "plt.suptitle('Receiver operating characteristic', fontsize=16)\n",
        "\n",
        "ax2.legend(loc=\"lower right\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9f3bb531dc762c7104f0115cf01673f3bb4a2711",
        "id": "bV3QHpkyw_da"
      },
      "cell_type": "markdown",
      "source": [
        "**ROC curve for both the model is sensible and indecates that both the model are equally powerful in terms of accuracy of prediction. AUC for the positive and negative reviews data point is more than 75% for both the model.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Review Prediction by Naive Bayes",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}